{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1511937a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py4j\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyspark 3.3.2 requires py4j==0.10.9.5, but you have py4j 0.10.9.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed py4j-0.10.9.7\n"
     ]
    }
   ],
   "source": [
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab8528e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/pyspark/__init__.py'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "637414ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 11:26:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa43d664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", 'true') \\\n",
    "    .csv('/home/joseun/data-engineering-zoomcamp/week_1_basics_n_setup/2_docker_sql/taxi+_zone_lookup.csv')\n",
    "\n",
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6811be91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb775da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_zones.write.parquet('zones', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "317fec75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-04 20:34:30--  https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet\n",
      "Resolving d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)... 52.85.114.180, 52.85.114.54, 52.85.114.114, ...\n",
      "Connecting to d37ci6vzurychx.cloudfront.net (d37ci6vzurychx.cloudfront.net)|52.85.114.180|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 394114750 (376M) [application/x-www-form-urlencoded]\n",
      "Saving to: ‘fhvhv_tripdata_2021-06.parquet’\n",
      "\n",
      "fhvhv_tripdata_2021 100%[===================>] 375.86M   154MB/s    in 2.4s    \n",
      "\n",
      "2023-03-04 20:34:32 (154 MB/s) - ‘fhvhv_tripdata_2021-06.parquet’ saved [394114750/394114750]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2021-06.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f0491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1278730 fhvhv_tripdata_2021-06.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhvhv_tripdata_2021-06.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f9da9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "june_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", 'true') \\\n",
    "    .parquet('fhvhv_tripdata_2021-06.parquet')\n",
    "\n",
    "june_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57aea18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 5, 31, 23, 53, 49), on_scene_datetime=datetime.datetime(2021, 6, 1, 0, 2, 23), pickup_datetime=datetime.datetime(2021, 6, 1, 0, 2, 41), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 7, 46), PULocationID=174, DOLocationID=18, trip_miles=1.0, trip_time=305, base_passenger_fare=8.13, tolls=0.0, bcf=0.24, sales_tax=0.72, congestion_surcharge=0.0, airport_fee=0.0, tips=0.0, driver_pay=7.03, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 6, 1, 0, 8, 40), on_scene_datetime=datetime.datetime(2021, 6, 1, 0, 15, 20), pickup_datetime=datetime.datetime(2021, 6, 1, 0, 16, 16), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 21, 14), PULocationID=32, DOLocationID=254, trip_miles=1.39, trip_time=298, base_passenger_fare=9.49, tolls=0.0, bcf=0.28, sales_tax=0.84, congestion_surcharge=0.0, airport_fee=0.0, tips=0.0, driver_pay=9.03, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 6, 1, 0, 15, 26), on_scene_datetime=datetime.datetime(2021, 6, 1, 0, 24, 48), pickup_datetime=datetime.datetime(2021, 6, 1, 0, 27, 1), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 42, 11), PULocationID=240, DOLocationID=127, trip_miles=5.51, trip_time=910, base_passenger_fare=19.61, tolls=0.0, bcf=0.59, sales_tax=1.74, congestion_surcharge=0.0, airport_fee=0.0, tips=0.0, driver_pay=15.34, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N'),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 6, 1, 0, 22, 29), on_scene_datetime=datetime.datetime(2021, 6, 1, 0, 44, 48), pickup_datetime=datetime.datetime(2021, 6, 1, 0, 46, 8), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 53, 45), PULocationID=127, DOLocationID=235, trip_miles=1.5, trip_time=457, base_passenger_fare=8.5, tolls=0.0, bcf=0.26, sales_tax=0.75, congestion_surcharge=0.0, airport_fee=0.0, tips=1.0, driver_pay=9.13, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag=' ', wav_request_flag='N', wav_match_flag='N'),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', originating_base_num=None, request_datetime=datetime.datetime(2021, 6, 1, 0, 42, 14), on_scene_datetime=None, pickup_datetime=datetime.datetime(2021, 6, 1, 0, 45, 42), dropoff_datetime=datetime.datetime(2021, 6, 1, 1, 3, 33), PULocationID=144, DOLocationID=146, trip_miles=5.072, trip_time=1071, base_passenger_fare=30.35, tolls=0.23, bcf=0.92, sales_tax=2.71, congestion_surcharge=2.75, airport_fee=0.0, tips=0.0, driver_pay=18.21, shared_request_flag='N', shared_match_flag='N', access_a_ride_flag='N', wav_request_flag='N', wav_match_flag='N')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "june_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7625e349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n",
      "/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hvfhs_license_num               object\n",
       "dispatching_base_num            object\n",
       "originating_base_num            object\n",
       "request_datetime        datetime64[ns]\n",
       "on_scene_datetime       datetime64[ns]\n",
       "pickup_datetime         datetime64[ns]\n",
       "dropoff_datetime        datetime64[ns]\n",
       "PULocationID                     int64\n",
       "DOLocationID                     int64\n",
       "trip_miles                     float64\n",
       "trip_time                        int64\n",
       "base_passenger_fare            float64\n",
       "tolls                          float64\n",
       "bcf                            float64\n",
       "sales_tax                      float64\n",
       "congestion_surcharge           float64\n",
       "airport_fee                    float64\n",
       "tips                           float64\n",
       "driver_pay                     float64\n",
       "shared_request_flag             object\n",
       "shared_match_flag               object\n",
       "access_a_ride_flag              object\n",
       "wav_request_flag                object\n",
       "wav_match_flag                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = june_df.limit(100)\n",
    "  \n",
    "# Converting DataFrame to pandas\n",
    "pandas_df = pandas_df.toPandas()\n",
    "\n",
    "pandas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcde9fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: string (nullable = true)\n",
      " |-- shared_match_flag: string (nullable = true)\n",
      " |-- access_a_ride_flag: string (nullable = true)\n",
      " |-- wav_request_flag: string (nullable = true)\n",
      " |-- wav_match_flag: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(pandas_df).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "775d5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "june_schema = types.StructType([\n",
    "        types.StructField('hvfhs_license_num', types.StringType(), True), \n",
    "        types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "        types.StructField('originating_base_num', types.StringType(), True), \n",
    "        types.StructField('request_datetime', types.TimestampType(), True), \n",
    "        types.StructField('on_scene_datetime', types.TimestampType(), True), \n",
    "        types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "        types.StructField('dropoff_datetime', types.TimestampType(), True), \n",
    "        types.StructField('PULocationID', types.LongType(), True), \n",
    "        types.StructField('DOLocationID', types.LongType(), True), \n",
    "        types.StructField('trip_miles', types.DoubleType(), True), \n",
    "        types.StructField('trip_time', types.LongType(), True), \n",
    "        types.StructField('base_passenger_fare', types.DoubleType(), True), \n",
    "        types.StructField('tolls', types.DoubleType(), True), \n",
    "        types.StructField('bcf', types.DoubleType(), True), \n",
    "        types.StructField('sales_tax', types.DoubleType(), True), \n",
    "        types.StructField('congestion_surcharge', types.DoubleType(), True), \n",
    "        types.StructField('airport_fee', types.DoubleType(), True), \n",
    "        types.StructField('tips', types.DoubleType(), True), \n",
    "        types.StructField('driver_pay', types.DoubleType(), True), \n",
    "        types.StructField('shared_request_flag', types.BinaryType(), True), \n",
    "        types.StructField('shared_match_flag', types.BinaryType(), True), \n",
    "        types.StructField('access_a_ride_flag', types.BinaryType(), True), \n",
    "        types.StructField('wav_request_flag', types.BinaryType(), True), \n",
    "        types.StructField('wav_match_flag', types.BinaryType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f02acb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: binary (nullable = true)\n",
      " |-- shared_match_flag: binary (nullable = true)\n",
      " |-- access_a_ride_flag: binary (nullable = true)\n",
      " |-- wav_request_flag: binary (nullable = true)\n",
      " |-- wav_match_flag: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "june_updated = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(june_schema) \\\n",
    "        .parquet('fhvhv_tripdata_2021-06.parquet')\n",
    "\n",
    "june_updated.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "094448a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 5, 31, 23, 53, 49), on_scene_datetime=datetime.datetime(2021, 6, 1, 0, 2, 23), pickup_datetime=datetime.datetime(2021, 6, 1, 0, 2, 41), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 7, 46), PULocationID=174, DOLocationID=18, trip_miles=1.0, trip_time=305, base_passenger_fare=8.13, tolls=0.0, bcf=0.24, sales_tax=0.72, congestion_surcharge=0.0, airport_fee=0.0, tips=0.0, driver_pay=7.03, shared_request_flag=bytearray(b'N'), shared_match_flag=bytearray(b'N'), access_a_ride_flag=bytearray(b' '), wav_request_flag=bytearray(b'N'), wav_match_flag=bytearray(b'N')),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 6, 1, 0, 8, 40), on_scene_datetime=datetime.datetime(2021, 6, 1, 0, 15, 20), pickup_datetime=datetime.datetime(2021, 6, 1, 0, 16, 16), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 21, 14), PULocationID=32, DOLocationID=254, trip_miles=1.39, trip_time=298, base_passenger_fare=9.49, tolls=0.0, bcf=0.28, sales_tax=0.84, congestion_surcharge=0.0, airport_fee=0.0, tips=0.0, driver_pay=9.03, shared_request_flag=bytearray(b'N'), shared_match_flag=bytearray(b'N'), access_a_ride_flag=bytearray(b' '), wav_request_flag=bytearray(b'N'), wav_match_flag=bytearray(b'N')),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 6, 1, 0, 15, 26), on_scene_datetime=datetime.datetime(2021, 6, 1, 0, 24, 48), pickup_datetime=datetime.datetime(2021, 6, 1, 0, 27, 1), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 42, 11), PULocationID=240, DOLocationID=127, trip_miles=5.51, trip_time=910, base_passenger_fare=19.61, tolls=0.0, bcf=0.59, sales_tax=1.74, congestion_surcharge=0.0, airport_fee=0.0, tips=0.0, driver_pay=15.34, shared_request_flag=bytearray(b'N'), shared_match_flag=bytearray(b'N'), access_a_ride_flag=bytearray(b' '), wav_request_flag=bytearray(b'N'), wav_match_flag=bytearray(b'N')),\n",
       " Row(hvfhs_license_num='HV0003', dispatching_base_num='B02764', originating_base_num='B02764', request_datetime=datetime.datetime(2021, 6, 1, 0, 22, 29), on_scene_datetime=datetime.datetime(2021, 6, 1, 0, 44, 48), pickup_datetime=datetime.datetime(2021, 6, 1, 0, 46, 8), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 53, 45), PULocationID=127, DOLocationID=235, trip_miles=1.5, trip_time=457, base_passenger_fare=8.5, tolls=0.0, bcf=0.26, sales_tax=0.75, congestion_surcharge=0.0, airport_fee=0.0, tips=1.0, driver_pay=9.13, shared_request_flag=bytearray(b'N'), shared_match_flag=bytearray(b'N'), access_a_ride_flag=bytearray(b' '), wav_request_flag=bytearray(b'N'), wav_match_flag=bytearray(b'N')),\n",
       " Row(hvfhs_license_num='HV0005', dispatching_base_num='B02510', originating_base_num=None, request_datetime=datetime.datetime(2021, 6, 1, 0, 42, 14), on_scene_datetime=None, pickup_datetime=datetime.datetime(2021, 6, 1, 0, 45, 42), dropoff_datetime=datetime.datetime(2021, 6, 1, 1, 3, 33), PULocationID=144, DOLocationID=146, trip_miles=5.072, trip_time=1071, base_passenger_fare=30.35, tolls=0.23, bcf=0.92, sales_tax=2.71, congestion_surcharge=2.75, airport_fee=0.0, tips=0.0, driver_pay=18.21, shared_request_flag=bytearray(b'N'), shared_match_flag=bytearray(b'N'), access_a_ride_flag=bytearray(b'N'), wav_request_flag=bytearray(b'N'), wav_match_flag=bytearray(b'N'))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "june_updated.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11795acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "def user_defined_timestamp(time):\n",
    "    time = time % (24 * 3600)\n",
    "    hour = time // 3600\n",
    "    time %= 3600\n",
    "    minutes = time // 60\n",
    "    time %= 60\n",
    "    seconds = time\n",
    "#     time = datetime.strptime(date_col, '%Y-%m-%d %H:%M:%S')\n",
    "    return f\"{hour:02d}:{minutes:02d}:{seconds:02d}\"\n",
    "        \n",
    "user_defined_timestamp_udf = F.udf(user_defined_timestamp, types.TimestampType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d672640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:05:05'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = user_defined_timestamp(305)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d349f6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 11:39:18 ERROR Executor: Exception in task 0.0 in stage 24.0 (TID 41)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "23/03/05 11:39:18 WARN TaskSetManager: Lost task 0.0 in stage 24.0 (TID 41) (de-zoomcamp.europe-north1-a.c.divine-catalyst-375310.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:552)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:505)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "\n",
      "23/03/05 11:39:18 ERROR TaskSetManager: Task 0 in stage 24.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10197/2239335979.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjune_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjune_updated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trip_timestamp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_defined_timestamp_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trip_time'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mjune_updated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/joseun/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 540, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.8 than that in driver 3.9, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n"
     ]
    }
   ],
   "source": [
    "june_updated = june_updated.withColumn('trip_timestamp', user_defined_timestamp_udf('trip_time'))\n",
    "june_updated.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "june_updated \\\n",
    "        .repartition(12) \\\n",
    "        .write.parquet('data/pq/fhvhv/2021/06/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac5ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|hvfhs_license_num|dispatching_base_num|originating_base_num|   request_datetime|  on_scene_datetime|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|trip_miles|trip_time|base_passenger_fare|tolls| bcf|sales_tax|congestion_surcharge|airport_fee|tips|driver_pay|shared_request_flag|shared_match_flag|access_a_ride_flag|wav_request_flag|wav_match_flag|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "|           HV0003|              B02764|              B02764|2021-05-31 23:53:49|2021-06-01 00:02:23|2021-06-01 00:02:41|2021-06-01 00:07:46|         174|          18|       1.0|      305|               8.13|  0.0|0.24|     0.72|                 0.0|        0.0| 0.0|      7.03|               [4E]|             [4E]|              [20]|            [4E]|          [4E]|\n",
      "|           HV0003|              B02764|              B02764|2021-06-01 00:08:40|2021-06-01 00:15:20|2021-06-01 00:16:16|2021-06-01 00:21:14|          32|         254|      1.39|      298|               9.49|  0.0|0.28|     0.84|                 0.0|        0.0| 0.0|      9.03|               [4E]|             [4E]|              [20]|            [4E]|          [4E]|\n",
      "|           HV0003|              B02764|              B02764|2021-06-01 00:15:26|2021-06-01 00:24:48|2021-06-01 00:27:01|2021-06-01 00:42:11|         240|         127|      5.51|      910|              19.61|  0.0|0.59|     1.74|                 0.0|        0.0| 0.0|     15.34|               [4E]|             [4E]|              [20]|            [4E]|          [4E]|\n",
      "|           HV0003|              B02764|              B02764|2021-06-01 00:22:29|2021-06-01 00:44:48|2021-06-01 00:46:08|2021-06-01 00:53:45|         127|         235|       1.5|      457|                8.5|  0.0|0.26|     0.75|                 0.0|        0.0| 1.0|      9.13|               [4E]|             [4E]|              [20]|            [4E]|          [4E]|\n",
      "|           HV0005|              B02510|                null|2021-06-01 00:42:14|               null|2021-06-01 00:45:42|2021-06-01 01:03:33|         144|         146|     5.072|     1071|              30.35| 0.23|0.92|     2.71|                2.75|        0.0| 0.0|     18.21|               [4E]|             [4E]|              [4E]|            [4E]|          [4E]|\n",
      "+-----------------+--------------------+--------------------+-------------------+-------------------+-------------------+-------------------+------------+------------+----------+---------+-------------------+-----+----+---------+--------------------+-----------+----+----------+-------------------+-----------------+------------------+----------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "june_updated.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d045455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|       day|taxi_trips|\n",
      "+----------+----------+\n",
      "|2021-06-15|    452470|\n",
      "+----------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "june_updated.createOrReplaceTempView('june_fhvhv')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATE(pickup_datetime) AS day,\n",
    "    COUNT(*) AS taxi_trips\n",
    "FROM\n",
    "    june_fhvhv\n",
    "WHERE\n",
    "    DATE(pickup_datetime) == \"2021-06-15\"\n",
    "GROUP BY\n",
    "    DATE(pickup_datetime)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f584e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|       day| hours_travelled|\n",
      "+----------+----------------+\n",
      "|2021-06-25|66.8788888888889|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATE(pickup_datetime) AS day,\n",
    "    (MAX(trip_time) / 3600) AS hours_travelled\n",
    "FROM\n",
    "    june_fhvhv\n",
    "GROUP BY\n",
    "    DATE(pickup_datetime)\n",
    "ORDER BY\n",
    "    hours_travelled DESC\n",
    "LIMIT\n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b251445",
   "metadata": {},
   "outputs": [],
   "source": [
    "june_zones_df = june_updated \\\n",
    "                            .join(df_zones, june_updated.PULocationID == df_zones.LocationID) \\\n",
    "                            .drop('LocationID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cb489d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 11:28:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "june_zones_df.write.parquet('data/pq/fhvhv/2021/06/revenue_zones', mode='overwrite') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc9a9c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- originating_base_num: string (nullable = true)\n",
      " |-- request_datetime: timestamp (nullable = true)\n",
      " |-- on_scene_datetime: timestamp (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- trip_time: long (nullable = true)\n",
      " |-- base_passenger_fare: double (nullable = true)\n",
      " |-- tolls: double (nullable = true)\n",
      " |-- bcf: double (nullable = true)\n",
      " |-- sales_tax: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      " |-- tips: double (nullable = true)\n",
      " |-- driver_pay: double (nullable = true)\n",
      " |-- shared_request_flag: binary (nullable = true)\n",
      " |-- shared_match_flag: binary (nullable = true)\n",
      " |-- access_a_ride_flag: binary (nullable = true)\n",
      " |-- wav_request_flag: binary (nullable = true)\n",
      " |-- wav_match_flag: binary (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "june_zones_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ef94117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+---------+\n",
      "|PULocationID|               Zone|frequency|\n",
      "+------------+-------------------+---------+\n",
      "|          61|Crown Heights North|   231279|\n",
      "+------------+-------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "june_zones_df.createOrReplaceTempView('june_zones')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    PULocationID,\n",
    "    Zone,\n",
    "    COUNT(1) AS frequency\n",
    "FROM\n",
    "    june_zones\n",
    "GROUP BY\n",
    "    PULocationID, Zone\n",
    "ORDER BY\n",
    "    frequency DESC\n",
    "LIMIT\n",
    "    1\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89482d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
